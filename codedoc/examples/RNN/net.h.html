<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>net.h</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="/custom.css" type="text/css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
<style>
h1, h2, h3 {
  margin: auto;
}
.flex-container {
  padding: 1rem;
  margin: auto;
  max-width: 1400px;
  display: flex;
  flex-wrap: wrap;
  justify-content: center;
}
.wide-text {
  flex: 0 1 900px;
  margin: auto;
}
.side-text, .code {
  flex: 0 0 40%;
  padding: 0 1em;
  overflow-x: scroll;
  scrollbar-width: thin;
}
@media all and (max-width: 900px) {
.flex-container > * {
  flex: 1 0 100%;
}
}
img {
  max-width: 100%;
}
video {
  max-width: 100%;
}
.wide-text video {
  max-width: 300px;
  margin: auto;
  display: block;
}
.code {
  background-color: rgba(116, 116, 116, 0.40);
}
.code > .sourceCode {
  overflow: unset;
}
</style>
</head>
<body>
<div class="flex-container">
<div class="wide-text">
<p>Implementing an Arduino Recurrent Neural Network</p>
</div>
<div class="side-text">

</div>
<div class="code">
<div class="sourceCode" id="cb1"><pre class="sourceCode cpp"><code class="sourceCode cpp"></code></pre></div>
</div>
<section id="linear-algebra-helpers" class="side-text">
<h4>Linear Algebra Helpers</h4>
</section>
<div class="code placeholder">

</div>
<div class="side-text">
<p><span class="math inline">\(\vec{\textrm{out}} = m\cdot\vec{\textrm{in}}\)</span>, matrix-vector multiplication</p>
</div>
<div class="code">
<div class="sourceCode" id="cb2"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb2-1"><a href="#cb2-1"></a><span class="dt">void</span> matvecmul(<span class="dt">float</span> m[], <span class="dt">float</span> in[], <span class="dt">float</span> out[], <span class="dt">size_t</span> <span class="va">s_in</span>, <span class="dt">size_t</span> <span class="va">s_out</span>) {</span>
<span id="cb2-2"><a href="#cb2-2"></a>  <span class="cf">for</span> (<span class="dt">size_t</span> i=<span class="dv">0</span>; i&lt;<span class="va">s_out</span>; i++) {</span>
<span id="cb2-3"><a href="#cb2-3"></a>    out[i] = <span class="dv">0</span>;</span>
<span id="cb2-4"><a href="#cb2-4"></a>    <span class="cf">for</span> (<span class="dt">size_t</span> j=<span class="dv">0</span>; j&lt;<span class="va">s_in</span>; j++) {</span>
<span id="cb2-5"><a href="#cb2-5"></a>      out[i] += m[i*<span class="va">s_in</span>+j]*in[j];</span>
<span id="cb2-6"><a href="#cb2-6"></a>    }</span>
<span id="cb2-7"><a href="#cb2-7"></a>  }  </span>
<span id="cb2-8"><a href="#cb2-8"></a>}</span></code></pre></div>
</div>
<div class="side-text">
<p><span class="math inline">\(\vec{\textrm{out}} = \vec{\textrm{out}} + m\cdot\vec{\textrm{in}}\)</span>, matrix-vector multiplication with accumulation</p>
</div>
<div class="code">
<div class="sourceCode" id="cb3"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb3-1"><a href="#cb3-1"></a><span class="dt">void</span> matvecmulacc(<span class="dt">float</span> m[], <span class="dt">float</span> in[], <span class="dt">float</span> out[], <span class="dt">size_t</span> <span class="va">s_in</span>, <span class="dt">size_t</span> <span class="va">s_out</span>) {</span>
<span id="cb3-2"><a href="#cb3-2"></a>  <span class="cf">for</span> (<span class="dt">size_t</span> i=<span class="dv">0</span>; i&lt;<span class="va">s_out</span>; i++) {</span>
<span id="cb3-3"><a href="#cb3-3"></a>    <span class="cf">for</span> (<span class="dt">size_t</span> j=<span class="dv">0</span>; j&lt;<span class="va">s_in</span>; j++) {</span>
<span id="cb3-4"><a href="#cb3-4"></a>      out[i] += m[i*<span class="va">s_in</span>+j]*in[j];</span>
<span id="cb3-5"><a href="#cb3-5"></a>    }  </span>
<span id="cb3-6"><a href="#cb3-6"></a>  }</span>
<span id="cb3-7"><a href="#cb3-7"></a>}</span></code></pre></div>
</div>
<div class="side-text">
<p><span class="math inline">\(\vec{\textrm{out}} = \vec{\textrm{a}} + \vec{\textrm{b}}\)</span>, vector-vector addition</p>
</div>
<div class="code">
<div class="sourceCode" id="cb4"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb4-1"><a href="#cb4-1"></a><span class="dt">void</span> vecvecadd(<span class="dt">float</span> a[], <span class="dt">float</span> b[], <span class="dt">float</span> out[], <span class="dt">size_t</span> s) {</span>
<span id="cb4-2"><a href="#cb4-2"></a>  <span class="cf">for</span> (<span class="dt">size_t</span> i=<span class="dv">0</span>; i&lt;s; i++) {</span>
<span id="cb4-3"><a href="#cb4-3"></a>    out[i] = a[i]+b[i];</span>
<span id="cb4-4"><a href="#cb4-4"></a>  }  </span>
<span id="cb4-5"><a href="#cb4-5"></a>}</span></code></pre></div>
</div>
<div class="side-text">
<p><span class="math inline">\(\vec{\textrm{out}} = \vec{\textrm{out}} + \vec{\textrm{a}}\)</span>, vector-vector addition with accumulation</p>
</div>
<div class="code">
<div class="sourceCode" id="cb5"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb5-1"><a href="#cb5-1"></a><span class="dt">void</span> vecvecaddinplace(<span class="dt">float</span> a[], <span class="dt">float</span> out[], <span class="dt">size_t</span> s) {</span>
<span id="cb5-2"><a href="#cb5-2"></a>  <span class="cf">for</span> (<span class="dt">size_t</span> i=<span class="dv">0</span>; i&lt;s; i++) {</span>
<span id="cb5-3"><a href="#cb5-3"></a>    out[i] += a[i];</span>
<span id="cb5-4"><a href="#cb5-4"></a>  }  </span>
<span id="cb5-5"><a href="#cb5-5"></a>}</span></code></pre></div>
</div>
<div class="side-text">
<p><span class="math inline">\(\vec{\textrm{out}} = \vec{\textrm{a}}\)</span>, copying a vector</p>
</div>
<div class="code">
<div class="sourceCode" id="cb6"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb6-1"><a href="#cb6-1"></a><span class="dt">void</span> veccopy(<span class="dt">float</span> a[], <span class="dt">float</span> out[], <span class="dt">size_t</span> s) {</span>
<span id="cb6-2"><a href="#cb6-2"></a>  <span class="cf">for</span> (<span class="dt">size_t</span> i=<span class="dv">0</span>; i&lt;s; i++) {</span>
<span id="cb6-3"><a href="#cb6-3"></a>    out[i] = a[i];</span>
<span id="cb6-4"><a href="#cb6-4"></a>  }  </span>
<span id="cb6-5"><a href="#cb6-5"></a>}</span></code></pre></div>
</div>
<div class="side-text">
<p><span class="math inline">\(\vec{\textrm{out}} = \textrm{relu}(\vec{\textrm{a}})\)</span>, applying the <span class="math inline">\(\textrm{relu}\)</span> function to every element of a vector. <code>RELU_MAX</code> is used to ensure that erroneous large values are trimmed and do not cause an overflow later in the computation. <span class="math inline">\(\textrm{relu}(x) = \begin{cases}  x &amp; \text{if } x &gt; 0, \\  0 &amp; \text{otherwise}. \end{cases}\)</span></p>
</div>
<div class="code">
<div class="sourceCode" id="cb7"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb7-1"><a href="#cb7-1"></a><span class="pp">#define RELU_MAX </span><span class="dv">30</span></span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="dt">void</span> relu(<span class="dt">float</span> a[], <span class="dt">float</span> out[], <span class="dt">size_t</span> s) {</span>
<span id="cb7-3"><a href="#cb7-3"></a>  <span class="cf">for</span> (<span class="dt">size_t</span> i=<span class="dv">0</span>; i&lt;s; i++) {</span>
<span id="cb7-4"><a href="#cb7-4"></a>    <span class="cf">if</span> (a[i]&gt;<span class="dv">0</span>) {</span>
<span id="cb7-5"><a href="#cb7-5"></a>      out[i] = a[i];</span>
<span id="cb7-6"><a href="#cb7-6"></a>      <span class="cf">if</span> (a[i]&gt;RELU_MAX) {out[i]=RELU_MAX;}</span>
<span id="cb7-7"><a href="#cb7-7"></a>    }</span>
<span id="cb7-8"><a href="#cb7-8"></a>    <span class="cf">else</span> {out[i] = <span class="dv">0</span>;}</span>
<span id="cb7-9"><a href="#cb7-9"></a>  }    </span>
<span id="cb7-10"><a href="#cb7-10"></a>}</span></code></pre></div>
</div>
<div class="side-text">
<p><span class="math inline">\(\vec{\textrm{a}} = \textrm{relu}(\vec{\textrm{a}})\)</span>, applying the <span class="math inline">\(\textrm{relu}\)</span> function inplace</p>
</div>
<div class="code">
<div class="sourceCode" id="cb8"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb8-1"><a href="#cb8-1"></a><span class="dt">void</span> reluinplace(<span class="dt">float</span> a[], <span class="dt">size_t</span> s) {</span>
<span id="cb8-2"><a href="#cb8-2"></a>  <span class="cf">for</span> (<span class="dt">size_t</span> i=<span class="dv">0</span>; i&lt;s; i++) {</span>
<span id="cb8-3"><a href="#cb8-3"></a>    <span class="cf">if</span> (a[i]&gt;<span class="dv">0</span>) {</span>
<span id="cb8-4"><a href="#cb8-4"></a>      <span class="cf">if</span> (a[i]&gt;RELU_MAX) {a[i]=RELU_MAX;}</span>
<span id="cb8-5"><a href="#cb8-5"></a>    }</span>
<span id="cb8-6"><a href="#cb8-6"></a>    <span class="cf">else</span> {a[i] = <span class="dv">0</span>;}</span>
<span id="cb8-7"><a href="#cb8-7"></a>  }    </span>
<span id="cb8-8"><a href="#cb8-8"></a>}</span></code></pre></div>
</div>
<div class="side-text">
<p><span class="math inline">\(\vec{\textrm{a}} = \textrm{softmax}(\vec{\textrm{a}})\)</span>, where <span class="math inline">\(\sigma(\vec{x})_i = \frac{e^{x_i}}{\sum_{j=1}^K e^{x_j}}\)</span> for <span class="math inline">\(\vec{x}=(x_1,\dotsc,x_K)\)</span>.</p>
</div>
<div class="code">
<div class="sourceCode" id="cb9"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb9-1"><a href="#cb9-1"></a><span class="dt">void</span> softmaxinplace(<span class="dt">float</span> a[], <span class="dt">size_t</span> s) {</span>
<span id="cb9-2"><a href="#cb9-2"></a>  <span class="dt">float</span> sum = <span class="dv">0</span>;</span>
<span id="cb9-3"><a href="#cb9-3"></a>  <span class="cf">for</span> (<span class="dt">size_t</span> i=<span class="dv">0</span>; i&lt;s; i++) {</span>
<span id="cb9-4"><a href="#cb9-4"></a>    a[i] = exp(a[i]);</span>
<span id="cb9-5"><a href="#cb9-5"></a>    sum += a[i];</span>
<span id="cb9-6"><a href="#cb9-6"></a>  }    </span>
<span id="cb9-7"><a href="#cb9-7"></a>  <span class="cf">for</span> (<span class="dt">size_t</span> i=<span class="dv">0</span>; i&lt;s; i++) {</span>
<span id="cb9-8"><a href="#cb9-8"></a>    a[i] /= sum;</span>
<span id="cb9-9"><a href="#cb9-9"></a>  }      </span>
<span id="cb9-10"><a href="#cb9-10"></a>}</span>
<span id="cb9-11"><a href="#cb9-11"></a></span></code></pre></div>
</div>
<section id="specifying-the-size-of-our-two-layer-recurrent-neural-network" class="side-text">
<h4>Specifying the size of our two-layer recurrent neural network</h4>
</section>
<div class="code">
<div class="sourceCode" id="cb10"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb10-1"><a href="#cb10-1"></a><span class="at">const</span> <span class="dt">size_t</span> neurons_in  = <span class="dv">6</span>; <span class="co">// input dimensions (3 acceleration and 3 rotation components)</span></span>
<span id="cb10-2"><a href="#cb10-2"></a><span class="at">const</span> <span class="dt">size_t</span> neurons_1s  = <span class="dv">6</span>; <span class="co">// first RNN memory size</span></span>
<span id="cb10-3"><a href="#cb10-3"></a><span class="at">const</span> <span class="dt">size_t</span> neurons_1o  = <span class="dv">6</span>; <span class="co">// first RNN output size</span></span>
<span id="cb10-4"><a href="#cb10-4"></a><span class="at">const</span> <span class="dt">size_t</span> neurons_2s  = <span class="dv">6</span>; <span class="co">// second RNN</span></span>
<span id="cb10-5"><a href="#cb10-5"></a><span class="at">const</span> <span class="dt">size_t</span> neurons_2o  = <span class="dv">6</span>; <span class="co">// ...</span></span>
<span id="cb10-6"><a href="#cb10-6"></a><span class="at">const</span> <span class="dt">size_t</span> neurons_out = <span class="dv">3</span>; <span class="co">// output dimensions (number of possible gestures)</span></span>
<span id="cb10-7"><a href="#cb10-7"></a></span></code></pre></div>
</div>
<section id="the-data-structures-used-to-store-the-parameters-for-our-recurrent-neural-network" class="side-text">
<h4>The data structures used to store the parameters for our recurrent neural network</h4>
</section>
<div class="code">
<div class="sourceCode" id="cb11"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb11-1"><a href="#cb11-1"></a><span class="dt">float</span> W1s[neurons_1s*neurons_in] = { <span class="fl">0.5321287</span>, <span class="fl">0.6107104</span>, <span class="fl">1.3286344</span>, -<span class="fl">0.6015093</span>, <span class="fl">0.56184286</span>, -<span class="fl">0.050672613</span>, -<span class="fl">0.28946915</span>, -<span class="fl">0.57202274</span>, <span class="fl">0.5797093</span>, <span class="fl">0.10561681</span>, <span class="fl">1.7290003</span>, -<span class="fl">0.68493843</span>, -<span class="fl">0.18255526</span>, -<span class="fl">0.37313738</span>, <span class="fl">1.2256309</span>, -<span class="fl">0.1742842</span>, <span class="fl">0.6492496</span>, -<span class="fl">0.16173245</span>, -<span class="fl">0.9695452</span>, -<span class="fl">0.5891188</span>, -<span class="fl">0.013323135</span>, <span class="fl">0.6650615</span>, <span class="fl">0.42116144</span>, -<span class="fl">0.59877527</span>, <span class="fl">0.058546405</span>, -<span class="fl">0.71124023</span>, -<span class="fl">0.4949681</span>, <span class="fl">0.026260667</span>, <span class="fl">0.47205865</span>, <span class="fl">0.017517323</span>, <span class="fl">0.44564795</span>, -<span class="fl">0.76846886</span>, <span class="fl">0.6713811</span>, -<span class="fl">1.7036608</span>, <span class="fl">0.06296093</span>, <span class="fl">0.27191502</span> };</span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="dt">float</span> U1 [neurons_1s*neurons_1s] = { <span class="fl">0.08446167</span>, <span class="fl">2.1222587</span>, -<span class="fl">0.262183</span>, <span class="fl">0.041411262</span>, <span class="fl">0.092383526</span>, -<span class="fl">1.0871025</span>, <span class="fl">0.765727</span>, -<span class="fl">1.2816284</span>, -<span class="fl">0.79608244</span>, <span class="fl">1.0392288</span>, <span class="fl">0.05335982</span>, <span class="fl">0.04486225</span>, <span class="fl">0.046529382</span>, -<span class="fl">1.4757994</span>, <span class="fl">0.13218978</span>, <span class="fl">0.0464745</span>, -<span class="fl">0.06651882</span>, -<span class="fl">0.8113239</span>, <span class="fl">2.010752</span>, <span class="fl">0.27113342</span>, -<span class="fl">0.082562454</span>, <span class="fl">0.36493123</span>, -<span class="fl">1.3630337</span>, <span class="fl">1.5551832</span>, -<span class="fl">1.8181484</span>, -<span class="fl">0.21996354</span>, -<span class="fl">0.60703295</span>, -<span class="fl">0.20818894</span>, <span class="fl">0.12968491</span>, <span class="fl">1.1444167</span>, -<span class="fl">1.0321264</span>, -<span class="fl">0.71735334</span>, -<span class="fl">0.4447773</span>, <span class="fl">0.25020576</span>, <span class="fl">0.4118029</span>, -<span class="fl">1.9402288</span> };</span>
<span id="cb11-3"><a href="#cb11-3"></a><span class="dt">float</span> W1o[neurons_1o*neurons_1s] = { <span class="fl">0.13802966</span>, -<span class="fl">1.2219762</span>, <span class="fl">0.058708522</span>, <span class="fl">0.1777648</span>, <span class="fl">1.6724335</span>, -<span class="fl">1.4119003</span>, -<span class="fl">0.032558057</span>, <span class="fl">0.44839227</span>, <span class="fl">0.40060967</span>, <span class="fl">0.64469165</span>, -<span class="fl">0.29333997</span>, -<span class="fl">0.025161164</span>, -<span class="fl">0.2800877</span>, -<span class="fl">0.902683</span>, -<span class="fl">0.007265689</span>, <span class="fl">0.54415846</span>, <span class="fl">0.66456676</span>, -<span class="fl">0.77940935</span>, <span class="fl">0.37354767</span>, <span class="fl">0.3158186</span>, -<span class="fl">0.6347513</span>, <span class="fl">0.17989126</span>, -<span class="fl">0.22734974</span>, -<span class="fl">0.49951622</span>, -<span class="fl">0.5184792</span>, <span class="fl">0.87216437</span>, <span class="fl">2.159849</span>, -<span class="fl">0.6910299</span>, -<span class="fl">0.08984194</span>, <span class="fl">0.032428134</span>, <span class="fl">0.13159314</span>, -<span class="fl">0.88068604</span>, <span class="fl">0.47105592</span>, <span class="fl">0.12390203</span>, -<span class="fl">0.3439282</span>, <span class="fl">0.4603399</span> };</span>
<span id="cb11-4"><a href="#cb11-4"></a><span class="dt">float</span> B1s[neurons_1s] = { -<span class="fl">1.105348</span>, <span class="fl">0.00018683162</span>, -<span class="fl">0.02873229</span>, -<span class="fl">0.06346775</span>, -<span class="fl">0.17056179</span>, <span class="fl">0.09518265</span> };</span>
<span id="cb11-5"><a href="#cb11-5"></a><span class="dt">float</span> B1o[neurons_1o] = { <span class="fl">0.34591475</span>, -<span class="fl">0.14795464</span>, <span class="fl">0.2325652</span>, -<span class="fl">0.10135709</span>, -<span class="fl">1.3309003</span>, <span class="fl">0.44049194</span> };</span>
<span id="cb11-6"><a href="#cb11-6"></a><span class="dt">float</span> W2s[neurons_2s*neurons_1o] = { -<span class="fl">0.47867066</span>, -<span class="fl">2.0650294</span>, -<span class="fl">0.09539489</span>, -<span class="fl">0.36280107</span>, <span class="fl">1.3170956</span>, <span class="fl">0.029860241</span>, -<span class="fl">0.27149627</span>, <span class="fl">0.58525115</span>, -<span class="fl">0.10355578</span>, -<span class="fl">0.44423324</span>, <span class="fl">0.232523</span>, -<span class="fl">0.6986958</span>, <span class="fl">0.082030736</span>, <span class="fl">0.055767298</span>, -<span class="fl">0.04935323</span>, -<span class="fl">0.28878313</span>, <span class="fl">0.066991135</span>, -<span class="fl">0.15354092</span>, <span class="fl">0.094942436</span>, -<span class="fl">1.6132945</span>, <span class="fl">0.036912344</span>, <span class="fl">0.29851636</span>, <span class="fl">1.2205178</span>, -<span class="fl">0.7134117</span>, -<span class="fl">1.2386477</span>, -<span class="fl">0.5278449</span>, -<span class="fl">0.19041571</span>, <span class="fl">0.048836123</span>, -<span class="fl">0.12733762</span>, <span class="fl">0.03632712</span>, <span class="fl">2.8797266</span>, <span class="fl">0.61762565</span>, <span class="fl">0.053293455</span>, -<span class="fl">0.34995812</span>, <span class="fl">0.3548257</span>, <span class="fl">0.8126559</span> };</span>
<span id="cb11-7"><a href="#cb11-7"></a><span class="dt">float</span> U2 [neurons_2s*neurons_2s] = { -<span class="fl">0.15002497</span>, -<span class="fl">0.7135592</span>, -<span class="fl">0.685698</span>, <span class="fl">0.19555788</span>, -<span class="fl">0.9222942</span>, <span class="fl">0.60537004</span>, <span class="fl">0.20041735</span>, -<span class="fl">0.015652642</span>, <span class="fl">0.2094619</span>, -<span class="fl">0.061256256</span>, <span class="fl">0.020260328</span>, <span class="fl">1.1851788</span>, <span class="fl">0.0739529</span>, <span class="fl">0.016536528</span>, <span class="fl">0.0017053923</span>, -<span class="fl">0.001012617</span>, <span class="fl">0.4333541</span>, <span class="fl">2.0129955e-05</span>, -<span class="fl">0.20776315</span>, <span class="fl">0.34800503</span>, <span class="fl">1.2278594</span>, -<span class="fl">0.2840473</span>, <span class="fl">1.6044365</span>, <span class="fl">0.11292993</span>, <span class="fl">1.2254378</span>, <span class="fl">2.0934033</span>, -<span class="fl">1.8157494</span>, <span class="fl">0.21263774</span>, -<span class="fl">0.009304846</span>, -<span class="fl">0.13362795</span>, <span class="fl">1.1646845</span>, -<span class="fl">0.19165255</span>, <span class="fl">0.011544819</span>, <span class="fl">1.0697566</span>, <span class="fl">0.43793356</span>, <span class="fl">0.09747889</span> };</span>
<span id="cb11-8"><a href="#cb11-8"></a><span class="dt">float</span> W2o[neurons_2o*neurons_2s] = { <span class="fl">0.014649042</span>, -<span class="fl">0.06266459</span>, <span class="fl">0.00044530613</span>, -<span class="fl">0.8122457</span>, -<span class="fl">0.36646405</span>, -<span class="fl">0.0798095</span>, <span class="fl">0.3709301</span>, <span class="fl">0.37230963</span>, <span class="fl">3.8917642e-05</span>, -<span class="fl">0.0005861422</span>, <span class="fl">0.2968952</span>, <span class="fl">1.5582066</span>, -<span class="fl">1.3202109e-05</span>, <span class="fl">1.799133</span>, -<span class="fl">1.5939819e-05</span>, <span class="fl">0.10810138</span>, <span class="fl">2.5070357</span>, <span class="fl">0.2853145</span>, -<span class="fl">1.0321325</span>, <span class="fl">0.0011201913</span>, <span class="fl">1.5661702e-05</span>, -<span class="fl">0.058740117</span>, <span class="fl">0.00024601142</span>, <span class="fl">0.19350934</span>, -<span class="fl">0.00044874512</span>, <span class="fl">0.7337828</span>, <span class="fl">5.3875847e-06</span>, <span class="fl">0.06492587</span>, <span class="fl">2.9979377</span>, -<span class="fl">0.56943846</span>, <span class="fl">0.00037777785</span>, -<span class="fl">0.00025590323</span>, -<span class="fl">0.0010820613</span>, <span class="fl">4.989095e-06</span>, <span class="fl">0.0007219418</span>, -<span class="fl">0.0005042125</span> };</span>
<span id="cb11-9"><a href="#cb11-9"></a><span class="dt">float</span> B2s[neurons_2s] = { -<span class="fl">0.028206171</span>, <span class="fl">0.5156735</span>, <span class="fl">6.478218e-05</span>, <span class="fl">1.2108356</span>, -<span class="fl">0.31679368</span>, -<span class="fl">0.024533587</span> };</span>
<span id="cb11-10"><a href="#cb11-10"></a><span class="dt">float</span> B2o[neurons_2o] = { <span class="fl">1.4028785</span>, <span class="fl">0.0001855492</span>, -<span class="fl">0.1894923</span>, <span class="fl">0.103634566</span>, -<span class="fl">0.113036916</span>, -<span class="fl">0.0009970091</span> };</span>
<span id="cb11-11"><a href="#cb11-11"></a><span class="dt">float</span> Wout[neurons_out*neurons_1o] = { -<span class="fl">1.5629566</span>, <span class="fl">0.4279106</span>, <span class="fl">0.21059136</span>, <span class="fl">0.0009926992</span>, <span class="fl">0.1298369</span>, <span class="fl">0.00050598645</span>, <span class="fl">2.0700727</span>, <span class="fl">0.19776538</span>, <span class="fl">0.18575023</span>, <span class="fl">0.48138946</span>, <span class="fl">0.9464396</span>, -<span class="fl">0.00084920716</span>, <span class="fl">0.053360283</span>, -<span class="fl">0.36190554</span>, -<span class="fl">1.7918297</span>, -<span class="fl">1.6159785e-05</span>, -<span class="fl">0.5657281</span>, <span class="fl">8.879579e-06</span> };</span>
<span id="cb11-12"><a href="#cb11-12"></a><span class="dt">float</span> Bout[neurons_out] = { -<span class="fl">1.3225704</span>, <span class="fl">3.2274518e-05</span>, <span class="fl">2.5265179</span> };</span>
<span id="cb11-13"><a href="#cb11-13"></a></span></code></pre></div>
</div>
<section id="global-arrays-in-which-to-store-the-input-and-output-vectors-as-well-as-intermediary-states" class="side-text">
<h4>Global arrays in which to store the input and output vectors, as well as intermediary states</h4>
</section>
<div class="code">
<div class="sourceCode" id="cb12"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb12-1"><a href="#cb12-1"></a><span class="dt">float</span> input[neurons_in];</span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="dt">float</span> state1[neurons_1s];</span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="dt">float</span> state1_tmp[neurons_1s];</span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="dt">float</span> hidden1[neurons_1o];</span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="dt">float</span> state2[neurons_2s];</span>
<span id="cb12-6"><a href="#cb12-6"></a><span class="dt">float</span> state2_tmp[neurons_2s];</span>
<span id="cb12-7"><a href="#cb12-7"></a><span class="dt">float</span> hidden2[neurons_2o];</span>
<span id="cb12-8"><a href="#cb12-8"></a><span class="dt">float</span> output[neurons_out];</span>
<span id="cb12-9"><a href="#cb12-9"></a></span></code></pre></div>
</div>
<section id="the-actual-neural-network" class="side-text">
<h4>The actual neural network</h4>
</section>
<div class="code">
<div class="sourceCode" id="cb13"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb13-1"><a href="#cb13-1"></a><span class="dt">size_t</span> network() {</span></code></pre></div>
</div>
<div class="side-text">
<p>First RNN layer <span class="math inline">\(\vec{s}_1 \leftarrow \sigma(W_{1s}\cdot\vec{s}_1+U_1\cdot\vec{v}_\textrm{in}+\vec{b}_{1r})\)</span></p>
</div>
<div class="code">
<div class="sourceCode" id="cb14"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb14-1"><a href="#cb14-1"></a>  matvecmul   (U1  , input  , state1_tmp, neurons_in, neurons_1s);</span>
<span id="cb14-2"><a href="#cb14-2"></a>  matvecmulacc(W1s , state1 , state1_tmp, neurons_1s, neurons_1s);</span>
<span id="cb14-3"><a href="#cb14-3"></a>  vecvecaddinplace(B1s, state1_tmp, neurons_1s);</span>
<span id="cb14-4"><a href="#cb14-4"></a>  relu(state1_tmp, state1, neurons_1s);</span>
<span id="cb14-5"><a href="#cb14-5"></a>  </span></code></pre></div>
</div>
<div class="side-text">
<p><span class="math inline">\(\vec{v}_\textrm{rec1} \leftarrow \sigma(W_{1o}\cdot\vec{s}_1+\vec{b}_{1o})\)</span></p>
</div>
<div class="code">
<div class="sourceCode" id="cb15"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb15-1"><a href="#cb15-1"></a>  matvecmul   (W1o , state1 , hidden1   , neurons_1s, neurons_1o);</span>
<span id="cb15-2"><a href="#cb15-2"></a>  vecvecaddinplace(B1o, hidden1, neurons_1o);</span>
<span id="cb15-3"><a href="#cb15-3"></a>  reluinplace(hidden1, neurons_1o);</span></code></pre></div>
</div>
<div class="side-text">
<p>Second RNN layer <span class="math inline">\(\vec{s}_2 \leftarrow \sigma(W_{2s}\cdot\vec{s}_2+U_1\cdot\vec{v}_\textrm{rec1}+\vec{b}_{2r})\)</span></p>
</div>
<div class="code">
<div class="sourceCode" id="cb16"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb16-1"><a href="#cb16-1"></a>  matvecmul   (U2  , hidden1, state2_tmp, neurons_1o, neurons_2s);</span>
<span id="cb16-2"><a href="#cb16-2"></a>  matvecmulacc(W2s , state2 , state2_tmp, neurons_2s, neurons_2s);</span>
<span id="cb16-3"><a href="#cb16-3"></a>  vecvecaddinplace(B2s, state2_tmp, neurons_2s);</span>
<span id="cb16-4"><a href="#cb16-4"></a>  relu(state2_tmp, state2, neurons_2s);</span>
<span id="cb16-5"><a href="#cb16-5"></a>  </span></code></pre></div>
</div>
<div class="side-text">
<p><span class="math inline">\(\vec{v}_\textrm{rec2} \leftarrow \sigma(W_{2o}\cdot\vec{s}_2+\vec{b}_{2o})\)</span></p>
</div>
<div class="code">
<div class="sourceCode" id="cb17"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb17-1"><a href="#cb17-1"></a>  matvecmul   (W2o , state2 , hidden2   , neurons_2s, neurons_2o);</span>
<span id="cb17-2"><a href="#cb17-2"></a>  vecvecaddinplace(B2o, hidden2, neurons_2o);</span>
<span id="cb17-3"><a href="#cb17-3"></a>  reluinplace(hidden2, neurons_2o);</span>
<span id="cb17-4"><a href="#cb17-4"></a>  </span></code></pre></div>
</div>
<div class="side-text">
<p>Final decision layer <span class="math inline">\(\vec{v}_\textrm{out} \leftarrow \textrm{softmax}(W\cdot\vec{v}_\textrm{rec2}+\vec{b})\)</span></p>
</div>
<div class="code">
<div class="sourceCode" id="cb18"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb18-1"><a href="#cb18-1"></a>  matvecmul   (Wout, hidden2, output    , neurons_2o, neurons_out);</span>
<span id="cb18-2"><a href="#cb18-2"></a>  vecvecaddinplace(Bout, output, neurons_out);</span>
<span id="cb18-3"><a href="#cb18-3"></a>  softmaxinplace(output, neurons_out);</span></code></pre></div>
</div>
<div class="side-text">
<p>Find and return the most “probable” gesture, i.e. the one with the largest component in the <code>output</code> vector.</p>
</div>
<div class="code">
<div class="sourceCode" id="cb19"><pre class="sourceCode cpp"><code class="sourceCode cpp"><span id="cb19-1"><a href="#cb19-1"></a>  <span class="dt">size_t</span> c = <span class="dv">0</span>;</span>
<span id="cb19-2"><a href="#cb19-2"></a>  <span class="cf">for</span> (<span class="dt">size_t</span> i=<span class="dv">0</span>; i&lt;neurons_out; i++) {</span>
<span id="cb19-3"><a href="#cb19-3"></a>    <span class="cf">if</span> (output[i]&gt;output[c]) {</span>
<span id="cb19-4"><a href="#cb19-4"></a>      c=i;</span>
<span id="cb19-5"><a href="#cb19-5"></a>    }</span>
<span id="cb19-6"><a href="#cb19-6"></a>  }</span>
<span id="cb19-7"><a href="#cb19-7"></a>  <span class="cf">return</span> c;</span>
<span id="cb19-8"><a href="#cb19-8"></a>}</span></code></pre></div>
</div>
</div>
</body>
</html>
